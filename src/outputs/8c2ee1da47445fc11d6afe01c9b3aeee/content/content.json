{
    "pdf_text": "SONUS TEXERE! AUTOMATED DENSE SOUNDTRACK CONSTRUCTION\nFOR BOOKS USING MOVIE ADAPTATIONS\nJaidev Shriram Makarand Tapaswi Vinoo Alluri\nInternational Institute of Information Technology, Hyderabad\nhttps://auto-book-soundtrack.github.io/\njaidev.shriram@students.iiit.ac.in, makarand.tapaswi@iiit.ac.in, vinoo.alluri@iiit.ac.in\nABSTRACT\nReading, much like music listening, is an immersive ex-\nperience that transports readers while taking them on an\nemotional journey. Listening to complementary music has\nthe potential to amplify the reading experience, especially\nwhen the music is stylistically cohesive and emotionally\nrelevant. In this paper, we propose the \ufb01rst fully automatic\nmethod to build a dense soundtrack for books, which can\nplay high-quality instrumental music for the entirety of the\nreading duration. Our work employs a unique text pro-\ncessing and music weaving pipeline that determines the\ncontext and emotional composition of scenes in a chap-\nter. This allows our method to identify and play relevant\nexcerpts from the soundtrack of the book\u2019s movie adap-\ntation. By relying on the movie composer\u2019s craftsman-\nship, our book soundtracks include expert-made motifs and\nother scene-speci\ufb01c musical characteristics. We validate\nthe design decisions of our approach through a perceptual\nstudy. Our readers note that the book soundtrack greatly\nenhanced their reading experience, due to high immersive-\nness granted via uninterrupted and style-consistent music,\nand a heightened emotional state attained via high preci-\nsion emotion and scene context recognition.\n1. INTRODUCTION\nIn 1975, a short repetitive piece of orchestral music gained\nnotoriety for inducing fear in its listeners, putting them in\na suspenseful state of impending doom. Today, it still in-\nduces the same reaction, right when the iconic shark of the\nmovie Jaws appears on screen. Movie composers, such as\nJohn Williams who created the Jaws theme, have long been\naware of the impact music has on its listeners. Well-placed\nmusic can accentuate scenes to raise the emotional stakes\nor foreshadow upcoming events; ill-suited music can even\nsuggest that something evil is afoot [1]. When taken as a\nwhole, the musical imagery afforded by such soundtracks\ngreatly complements the movie watching experience. In\nthis paper, we attempt to answer whether we can create a\nsimilar experience for books ( cf. Fig. 1).\n\u00a9 J. Shriram, M. Tapaswi, and V . Alluri. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: J. Shriram, M. Tapaswi, and V . Alluri, \u201cSonus Texere! Au-\ntomated Dense Soundtrack Construction for Books using Movie Adapta-\ntions\u201d, in Proc. of the 23rd Int. Society for Music Information Retrieval\nConf., Bengaluru, India, 2022.\nFigure 1 . We aim to transport readers to a musical universe\nby building a dense and coherent book soundtrack that is\nborrowed from movie adaptations.\nNot unlike movies, reading literature can be an incredi-\nbly transportive process that puts one in a meditative state,\nwhile actively engaging their imagination and mind [2].\nApart from the lack of visuals, books share many similari-\nties from a music composer\u2019s perspective: they have recur-\nring themes, characters (allowing for unique leitmotifs [3]),\nand even long-drawn emotional and narrative arcs that can\nbe teased and reinforced musically. Still, it is uncommon\nto see soundtracks that are tailored for books; it is up to\nthe reader to curate their own playlist and craft their ex-\nperience. Inconvenience aside, this requires the reader to\npreempt the type of music expected for a book\u2019s chapter\nand switch songs at relevant plot points. This is simply in-\nfeasible. In this work, we resolve these issues by proposing\nthe \ufb01rst automatic system to build a dense soundtrack that\nplays throughout the entire reading duration of a book.\nSpeci\ufb01cally, we focus on building a soundtrack for\nbooks with movie adaptations. This allows us to draw on\nthe corresponding movie soundtrack and take advantage\nof the composer/director\u2019s musical intents and instincts for\nthesame story . However, adapting the soundtrack is far\nfrom trivial due to missing alignment between book parts\nand music segments. Our approach resolves this by \ufb01nding\nscenes in the movie adaptation that match parts of the book\nand searching for music that plays in that movie scene.\nThis results in high quality, narrative speci\ufb01c matches that\namplify the reading experience. We conduct our experi-\nments on Harry Potter and the Philosopher\u2019s Stone , cho-arXiv:2212.01033v1  [cs.SD]  2 Dec 2022sen due to its popularity and Academy Award nominated\nsoundtrack. We evaluate the quality of our soundtrack by\nasking participants to read two music-accompanied chap-\nters of the book followed by a semi-structured interview.\nOur contributions can be summarised as follows: (i) We\npropose the \ufb01rst fully automatic approach for constructing\nbook-long soundtracks. (ii) Our pipeline matches the story\nin the book and movie to lift musical cues from the movie\nadaptation. Gaps in this alignment are \ufb01lled through an\nemotion-driven music retrieval system. (iii) A perceptual\nstudy of the soundtrack validates our proposed approach\nand provides further insights into soundtracking for books.\n2. RELATED WORK\nSoundtracking for books. Methods for constructing book\n(or story) soundtracks can be divided into two: generative\nor retrieval based. Generative approaches typically parse\nthe text for concepts and provide that as input to a music\ngeneration pipeline. Topic extraction followed by senti-\nment analysis [4] or density estimation of emotion-related\nwords [5] are used to generate melodies. On the other\nhand, retrieval based approaches, mine text for semantic\nconcepts that are used to retrieve ambient music (some-\ntimes with pitch correction) [6] or apply similar ideas to\nTwitter texts [7]. The general idea of cross-modal text-\nto-audio retrieval [8] is adapted for tag-based music [9]\nor characterises documents and music as a distribution\nover emotions [10]. Recently, Won et al. [11] propose a\njoint emotion-driven embedding space for story sentences\nand music enabling cross-modal retrieval. However, un-\nderstanding emotions conveyed in a book depends on the\nlarger narrative requiring a context of more than a few sen-\ntences. Thus, while [11] could be used to construct dense\nbook soundtracks in theory, the potential switching of mu-\nsic and resulting change in emotion every few sentences\nmay not result in a good user experience.\nUnlike above approaches, we focus on retrieving mu-\nsic from the soundtrack of a book\u2019s movie adaptation. Our\napproach assembles coherent music for books by consid-\nering large chunks of the book chapter, aligning them with\nmovie scenes, and \ufb01nding matched music pairs. We \ufb01ll\nin the gaps (only unmatched chapter segments) by relying\non emotion-based matching. To the best of our knowledge,\nwe are the \ufb01rst to perform narrative-speci\ufb01c music retrieval\nandweave a soundtrack for the entire book.\nMulti-modal music recommendation systems use cues\nsuch as user location, time, and environmental information\nto play music in day-to-day life [12] or even traf\ufb01c condi-\ntions for music in cars [13]. Perhaps closest to our work,\nPICASSO is a \ufb01ne example of a ranking model trained on\npairs of matching music and movie-clips (images, subti-\ntles) that is used to provide music recommendations for\nimage slideshows or audio books [14]. Instead, our work\nfocuses on producing a dense book soundtrack by aligning\nnarrative components of the book with the movie adapta-\ntion. This results in high precision narrative matches and\nensures a coherent soundtrack adapted from the same story.3. ASSEMBLING SOUNDTRACKS FOR BOOKS\nA large book may take several (variable) hours for a reader\nto complete. Within the book, there are multiple chapters,\neach having sections indicated by the theme, emotion, or\nlocation. An ideal soundtrack should respect these scene\nboundaries and change accordingly. On the music front,\nwhile different tracks from the soundtrack are typically ho-\nmogeneous, they are rarely played in their entirety in the\nmovie at a single stretch. A track may include the score for\nthe setup, con\ufb02ict, climactic moment, and resolution for a\ncertain storyline, which composers will selectively play at\nopportune moments to maximise emotional payoffs.\nWe use the movie adaptation as an intermediary be-\ntween the two that links plot points from the book with\nsnippets of music from the soundtrack album. We \ufb01rst di-\nvide each modality - the book, music soundtrack, and the\nmovie into smaller segments (Sec. 3.1). Then, we obtain\nclues about the soundtrack associated with each plot point\nin the book by aligning both the book and soundtrack to\nthe movie (Sec. 3.2). Finally, we weave together the mu-\nsic for the book by combining movie-based and emotion-\nbased matches (Sec. 3.3). Fig. 2 illustrates this overall \ufb02ow.\n3.1 Segmenting the Book, Music, and Movie\nWe discuss methods for segmenting the book chapters Bi\nin a bookB= [B1;:::;BL]; identifying cohesive musi-\ncal segments within each track Mjof the albumM=\n[M1;:::;M P]; and identifying scene boundaries for a\nmovieV= [V1;:::;V Q]that facilitates the alignment.\nBook narrative segmentation. We divide the text in a\nbook chapter based on narrative-relevant factors such as\ntheme, location, activity composition, character constella-\ntion, or even time [15]. Due to the absence of large datasets\nfor this task, we adopt an unsupervised approach, tempo-\nrally weighted hierarchical clustering (TW-FINCH) [16],\nrecently shown to be successful on video activity segmen-\ntation. For a chapter Bi, we encode each paragraph us-\ning a pretrained language model \u001eLM(\u0001)(speci\ufb01cally MP-\nNet [17] that performs well for sentence embeddings and\nsemantic search [18]), and cluster semantically similar and\nspatially close paragraphs to produce disjoint partitions,\nfBp\nigKi\np=1=TW-FINCH (\u001eLM(Bi))): (1)\nWe align individual segments Bp\niwith music segments.\nWe also considered a few baselines but ignored them\ndue to inferior results. TextTiling [19] tended to uniformly\npartition chapters while TopicTiling [20] often resulted in\nover-segmentation. In contrast, our approach yielded seg-\nments that mostly respected narrative shifts.\nKeystrength based music segmentation. As mentioned\nearlier, the entire track Mjfrom an album is (almost) never\nplayed in a portion of the movie. Hence, similar to book\nchapters, we focus on creating homogeneous emotionally-\ncohesive track segments that can be played continuously.\nA key feature that music segments are required to re-\n\ufb02ect are emotions that the director/composer intends to\nconvey. Since we expect homogeneity in music segmentBook Movie Music\n1. Segment all modalities to obtain homogenous entitites\n1. The Arrival of \nBaby Harry.mp3\n2. Visit to the Zoo \nand Letters from \nHogwarts.mp3\n3. Diagon Alley and \nGringotts Vault.mp3\n...Original \nSoundtrack\nAlbumBook Chapters\nMovie Scenes Full Length Audio Track\n2. Align the book to the movie and movie to the soundtrack\n\"It's obvious, isn't it?\" said Ron. \n\"We've got to play our way \nacross the room.\" \"We've got to play our way \nacross the room.\"\nRon WeasleyBehind the \nwhite \npieces they could \nsee another door.The Chess Game.mp3Positive Negative\n3. Build soundtrack using movie and emotion cuesFigure 2 .Overview of approach. We present a novel technique to build a musically rich book-length soundtrack. We\naccomplish this by \ufb01rst segmenting the book, its adaptation, and music into smaller homogenous chunks. These segments\nare then matched with the movie acting as an intermediary for text and music, thereby producing the \ufb01nal soundtrack.\nemotions, keystrength [21] is a good attribute as it captures\nthe tonal properties of music, and tonal changes likely sug-\ngest emotional shifts ( e.g. major to minor mode suggesting\na potential shift from positive to negative valence [22]).\nKeystrength as extracted via the MIRToolbox [23] gives us\na probability for each possible key resulting in a 24 dimen-\nsional vector (12 major and 12 minor keys). We use a win-\ndow size of 10 seconds with an 85% overlap, that helps\nignore minor local variations in the track while retaining\nlarger shifts. We then compute a self-similarity matrix [24]\nof the time-varying keystrength vector which is then used\nto calculate the novelty curve, with a kernel size of 64 [24].\nThe peaks in the novelty curve determine points of change.\nFinally, we use those peaks as our segment boundaries to\nproduceLjmusic segmentsfMc\njgLj\nc=1(see Fig. 3).\nMovie scene detection. As a third step, we segment the\nmovie into narrative-coherent scenes [V1;:::;V Q]using\nthe approach described in [25]. First, the movie is divided\ninto shots (consecutive video frames from the same cam-\nera). Then, a dynamic programming algorithm is used to\n\ufb01nd scene boundaries (a scene consists of multiple shots)\nso as to maximise intra-scene similarity.\nSummarising, we denote Bp\nias the segment of book\nchapterBi;Mc\njas the music segment from track Mj; and\nVqas theqthscene from the movie V.\n3.2 Aligning the Book, Movie, and Music\nWe \ufb01rst align the book with the movie adaptation using a\nnew two-stage coarse-to-\ufb01ne alignment scheme. Then, we\nalso align the movie audio to the soundtrack album.Chapter-scene coarse alignment. We \ufb01rst assign a set of\nscenes from the movie to each book chapter. We use an ap-\nproach similar to [26] where pairwise similarities are com-\nputed between a book chapter Biand a video scene Vqvia\ncharacter histograms and matched dialogues. The chapter-\nscene relationship is then encoded as a graph (each node\nrepresents a chapter-scene pair), with edge weights repre-\nsenting similarity scores. Calculating the shortest path over\nthis graph provides the alignment between all chapters and\nscenes. Additional details are provided in the Appendix.\nParagraph-scene re\ufb01nement. The coarse alignment can-\nnot be used directly for soundtracking as a chapter Bicon-\ntains distinct segments Bp\nithat likely need different mu-\nsic. Thus, in addition to sparse dialogue matches, we com-\npute similarities between sentences in the chapter segment\nBp\niand frames of the video scene Vqusing a pretrained\nvision-language model (CLIP [27]). To improve the qual-\nity of CLIP matching scores, we prune dialog and mundane\nsentences from the chapter segment using a TF \u0001IDF based\nscoring system. This emphasises relatively rare characters\nand objects that are likely to give stronger matches than\ncommonly occurring ones. We also retain sentences with\na high concreteness index [28] that measures how likely a\nword can be seen or experienced (in contrast to abstract\nwords). Finally, we take the top remaining sentences, en-\ncode them with CLIP, and calculate cosine similarity with\nall CLIP encoded video shot frames in the chapter\u2019s scenes\n(see Fig. 4). Scenes with a score higher than \u0012are assigned\nto the text segment using a mapping function,\nA(Bp\ni) =fVq:CLIP (Bp\ni;Vq)>\u0012g: (2)020406080100120140160180\ntime axis (in s.)CMC#MDMD#MEMFMF#MGMG#MAMA#MBMCmC#mDmD#mEmFmF#mGmG#mAmA#mBmtonal centerKey strength\n Similarity matrix\n20406080100120140160180\ntemporal location of frame centers (in s.)20406080100120140160180temporal location of frame centers (in s.)0 20 40 60 80 100 120\nTemporal location of events (in s.)00.10.20.30.40.50.60.70.80.91coefficient valueNovelty Curve\n-0.6-0.4-0.200.20.40.60.81Figure 3 .Music segmentation pipeline. We segment all tracks from the soundtrack to ensure a cohesive listening expe-\nrience. We extract keystrength [21] that captures tonal properties of a soundtrack (left), compute the self-similarity matrix\n(center), and use that to calculate the novelty curve [24]. The peaks of this curve are used to segment the track.\nAligning the movie audio track with the soundtrack al-\nbum. We identify the music in the movie by passing its au-\ndio track through Shazam1, a popular commercial audio-\n\ufb01ngerprint based search application [29] using its free pub-\nlic API. This results in high-precision estimates for the mu-\nsic played every few seconds. While any audio \ufb01ngerprint-\ning approach would perhaps work, we found that Shazam\nwas quite accurate in the presence of background noise and\ndialogue, which is pervasive in movies.\n3.3 Weaving the Book Soundtrack\nPost alignment with the movie, book chapter segments Bp\ni\nbelong to one of two sets: (i) those associated with at least\none movie sceneS=fBp\ni:jA(Bp\ni)j\u00151g; and (ii) those\nwithout, \u0016S=fBp\ni:A(Bp\ni) =?g. Recall,A(Bp\ni)is the set\nof aligned video scenes to a book segment ( cf. Eq. 2).\nExtracting emotion labels. For all text segments Bp\ni, we\nclassify each paragraph into positive, neutral, or negative\nusing a BERT-based emotion classi\ufb01er \u001eBERT, trained on\nReddit comments [30]. A majority vote across paragraphs\nis used to assign the emotion label to the chapter seg-\nmentEbook(Bp\ni) = mode (\u001eBERT(Bp\ni)). For a music seg-\nment, similar to [5], we encode it\u2019s emotion as valence,\nEmusic(Mc\nj), based on the mode of the song (major or mi-\nnor). This is based on literature that indicates that tracks\nin minor tend to be associated with negative emotions and\ntracks in major with positive emotions [22]. We also tried\napproaches that predict emotion from audio [11, 31], but\nthey didn\u2019t work as well for our application.\nImporting music snippets from the video scene. For\nevery text segment Bp\ni2S, we extract the movie times-\ntamps corresponding to the matched dialogues or CLIP-\nbased frame-sentence pairs. We use the audio-search to\nidentify the overall track Mjbeing played at any of the\nabove timestamps in the movie (in a small neighbourhood).\nA speci\ufb01c music segment Mc\njis chosen by matching emo-\ntion predictions i.e.Ebook(Bp\ni) =Emusic(Mc\nj)(we pick one\nrandomly if there are several segments). While we can\npick emotion-matching music segments without the book-\nmovie alignment, it will likely result in spurious matches.\nEmotion-based retrieval. For the chapter segments that\n1https://www.shazam.com/are not aligned with any video scene, \u0016S, and those inS\nthat did not \ufb01nd an emotion compatible soundtrack, we as-\nsign a random music segment among the set of emotionally\ncompatible compositions. Note that while we can pick any\nmusic segment (even from different movies), we restrict to\nthe soundtracks for this movie maintaining the composer\u2019s\nstylistic coherence.\n4. RESULTS AND EV ALUATION\nOur approach is designed to be applicable to books that\nhave mostly faithful movie adaptations in terms of few\nmatching dialogues, a relatively similar plot, and at least\nsome matching characters. We evaluate it on the \ufb01rst book\nand movie pair in the Harry Potter series, Harry Potter and\nthe Philosopher\u2019s Stone .\n4.1 Harry Potter: A Case Study\nOur unsupervised text segmentation approach splits 17\nchapters into 87 segments, with an average of 5.11 seg-\nments per chapter. Assuming a reading speed of 250 words\nper minute, each segment requires a ~4 minute track, for a\ntotal soundtrack duration of ~6 hours. Music segmentation\nThe snake jerked its head t oward\nUncle Vernon and Dudle y, then \nraised its e yes to the ceiling.\nfolded up his newspa per, and \nthey cla mbered up the  steps \nonto the street.\nRon turned the binoculars back \non Harr y.\nHagrid\nstone\nFigure 4 .CLIP-based paragraph-scene alignment.\nAligned examples of automatically selected visual sen-\ntences and their video frames in the scene using the vision-\nlanguage model CLIP [27] in a zero-shot setting. Text\nhighlights are words that we think that caused the match.1234567891011121314151617\nChapter Number0246810Number of Text SegmentsSegments with Emotion-Retrieved Music\nSegments with Movie-Retrieved Music\nFigure 5 .Number of chapter segments with movie- vs.\nemotion-retrieved music. Several chapters in the book\nare well represented in the movie and are predominantly\nsoundtracked using movie cues. For others, we use an\nemotion-based retrieval method to build a soundtrack.\nof 19 tracks from the soundtrack album results in 47 audio\nchunks with an average duration of ~52 seconds. For the\nmovie, we obtain 120 scenes using the scene detection al-\ngorithm. Computing the book-movie alignment results in\na strong chapter-scene alignment, with 82% accuracy, cal-\nculated as the percentage of movie shots correctly assigned\nto a chapter [26].\nFor the \ufb01ne-grained alignment, 34 chapter segments\nhave dialogue matches, while the CLIP based re\ufb01nement\nresults in a 130% improvement in the number of chapter\nsegments associated with the movie; only 9 segments re-\nmain unmatched. After performing the \ufb01nal book-music\nalignment, 45 chapter segments have music imported from\na matching video scene through the book-movie align-\nment. The remainder 42 segments are soundtracked using\nmusic segments fetched via emotion matching. See Fig. 5\nfor a chapter-wise distribution.\n4.2 Experiment Design\nAs the primary goal of our method is to improve the read-\ning experience, we obtain user feedback by setting up\nsemi-structured interviews with 10 individuals, who each\nread two chapters of the Harry Potter book. We randomly\npicked one chapter for each person while the \ufb01nal book\nchapter was read by all. We chose the \ufb01nal chapter as it\nrepresents all aspects of our method: it includes music re-\ntrieved via movie cues as well as pure emotion based re-\ntrieval, apart from being a key chapter in terms of the plot.\nThis allows us to fairly evaluate the effectiveness of our\nsoundtrack at a book level as reading these chapters takes\naround an hour for each participant.\nAll participants are asked to read these chapters consec-\nutively and answer a series of questions about several as-\npects of the reading experience. No user data is collected\nthrough this process and user consent is collected prior to\nrecording the interviews. The study was also authorised by\nthe author\u2019s institutional ethics committee.\nReading application. A bottleneck with playlist based ap-proaches for soundtracking books is that they require user-\ninput to transition at appropriate instants. We resolve this\nby creating an application that plays music in the back-\nground of the displayed text. Our application loops the\nmusic segment in\ufb01nitely and cross-fades to the next as the\nreader moves on to the next text segment. This ensures\ncomplete immersiveness and lets participants with varying\nreading speeds truly enjoy the soundtrack. For readers to\nhave a \ufb01rst-hand experience, we package this application\nfor a few chapters at the project page.\nParticipant information. A total of 10 individuals (18-\n22 age range, 6 male, 4 female) volunteered for the study.\nEach participant had previously read the Harry Potter book\nwithout music and happened to be familiar with the movie\nas well. We also provided each participant with a small\nmonetary reward as compensation for their time.\n4.3 Findings\nThe soundtrack improved immersiveness. All partici-\npants reported that the soundtrack improved the immer-\nsiveness of the reading experience. This is remarkable as\nall participants noted that they typically do not listen to mu-\nsic while reading. Some participants were more receptive\nto the soundtrack than others and spoke glowingly about\nthe movie-like immersion afforded by the music. These\nparticipants could recognise that the music played was\nsimilar to the score at the relevant movie scene. Others,\nwho were positive but less movie-inclined, focused more\non how the music \"set the environment\" (P4). Phrases such\nas\"set the mood\" ,\"intensi\ufb01ed the emotion\" ,\"\ufb01t the vibe\" ,\nand\"enhanced the experience\" were common among such\nparticipants.\n\"The music provided insight into the tone of\nthe chapter and [...] beyond imagination, pro-\nvided a soundtrack to what was read\" (P1)\nSurprisingly, some participants reported that the \ufb01rst sec-\ntion of the random chapter that they read \ufb01rst threw them\noff initially, suggesting that there is an adjustment period\nto this experience. These participants reported that they\nwere very comfortable with later sections ( \"after the \ufb01rst\n10-12 lines\" - P2) once they had gotten into a reading \ufb02ow.\nThe soundtrack helped visualise the book. Some partic-\nipants could recognise that the music played was similar\nto the score at the relevant movie scene. These partici-\npants were especially appreciative of the soundtrack and\nstated that it helped \"visualise the book, with movie like\nvisuals\" (P1). Such participants typically appreciated the\nmusic\u2019s cohesiveness and were likely referring to the pleas-\ning soundscape laid out by our soundtrack. One participant\nexplicitly pointed out that the soundtrack helped visualise\ncharacter interactions and that without, it would have just\nbeen \"two characters talking\" .\nMany were also receptive to the recurring motifs and\nthemes that appeared throughout, such as the central Harry\nPotter theme and stated that it helped imagine the \ufb01ctionalworld. Only one participant, P3, expressed complete dis-\nagreement with the music played in a text segment, for\nironically the same reason, stating that the signature Harry\nPotter motif distracted them. Barring this, the same person\nspoke warmly about the remaining soundtrack, suggesting\nthat the overtness of the motif, which permeates culture\ntoday, may be subject to individualistic preferences.\nMusic helped focus.\n\"I get distracted when reading so it helped me\nfocus on certain parts\" (P5)\nWhen describing the immersiveness of the reading expe-\nrience, three participants speci\ufb01cally pointed out that the\nmusic helped them read continuously, without distractions\nthat are typically present when reading in the absence of a\nsoundtrack. It should be noted that few participants who\ndescribed an aversion for listening to music during typical\nrecreational reading speci\ufb01cally pointed out that they avoid\npop music, suggesting that instrumental music, in general,\nmay be better suited for reading purposes.\nModerate repetition is not a concern. Most participants\nnoticed repetitive music in some text segments that they\nread but only brought it up when explicitly asked. One par-\nticipant suggested playing a different, similarly composed\ntrack instead of repeating the music, but in agreement with\nthe rest of the pool, felt that repetition did not affect the\nexperience. These participants also said that the repetition\nwasn\u2019t glaringly obvious and that it did not distract or take\naway from the immersion. It should be noted that repeti-\ntion refers to the same track being played on a cross-fade\nloop while a person reads a text segment, due to variable\nreading speeds. Despite this, it is noteworthy that our mu-\nsical segments are homogeneous enough to be played re-\npeatedly and that the text segments are narratively cohesive\nto warrant such repetition. When speci\ufb01cally asked about\nthe diversity of the music played, all participants expressed\npositive opinions and noted that the tracks kept changing as\nand when required.\nNarrative transitions mirrored in music. Many partic-\nipants engaged in a conversation with the authors post-\ninterview about how the soundtrack was built. All such\nparticipants were startled by the fact that the entire process\nwas completely automatic. Their surprise was primarily\ndue to the fact that the music was automatically matched\nwith relevant areas in text and that it transitioned at appro-\npriate narrative points.\n\"It actually made the experience better as the\ntransition put you in the mood for the expected\nemotion - from melancholy to sad.\" (P9)\nWhen asked speci\ufb01cally about these narrative transitions\nduring the interview, all participants agreed that it was\nemblematic of emotional or narrative shifts. Since the\nlast chapter was common across all participants, there was\nstrong consensus about the narrative shifts here especially,\nwith participants noting that the music increased in tension\nas the \ufb01nal hero-villain clash developed and that it eased\ninto more mellow, tender music once it was resolved.\"When the tension built in the plot, the music\ntransitioned to match it.\" (P1)\nSome participants explicitly appreciated the foreshadow-\ning made possible by the music, as suspenseful music at the\nstart of a segment would precede a similar narrative plot-\nline. Others simply elaborated on their earlier comments\nabout the immersiveness and re-emphasised the high con-\ngruency between text segments and the matched music.\nLow-arousal music preferred. We asked participants to\ndescribe the emotional suitability of the music, speci\ufb01cally,\nand in line with prior answers, and received a favourable\nresponse. The few critiques that were present revolved\naround the energy/arousal of the music played at certain\ninstances. Some participants were dissatis\ufb01ed with seg-\nments that were high in arousal, even though the valence\nmatched, as it broke the immersion. They described how\nthis music distracted them from reading and drew too much\nattention to the track itself. This is likely an important fac-\ntor to consider for future work on soundtracking books.\nMovie-based music cues stronger than emotion-based\ncues. At the end of the interview, participants were asked\nto describe their favourite and least favoured pieces of mu-\nsic from the two chapters, if any. The best segments were\nsplit between music retrieved via movie cues and those re-\ntrieved via emotion matching, with the former being more\nprominent. On the other hand, the least favoured segments,\noften mentioned only after the author\u2019s insistence, were\ntypically emotion-based matches that were described in\nterms of their neutrality. This \ufb01nding suggests that our ap-\nproach can effectively retrieve narrative-speci\ufb01c music and\nthat such a system is perhaps well-suited for book sound-\ntracking, as opposed to pure emotion-based methods.\n5. CONCLUSION\nWe presented a novel system to automatically weave a\nbook-length soundtrack, using the music present in the rel-\nevant movie adaptation. Perceptual validation of the con-\nstructed soundtrack for the \ufb01rst book and movie pair of the\nHarry Potter series provided very positive feedback that\nvalidated several proposed design decisions and techniques\nfor text, movie, and music processing. Participants in our\nperceptual study were particularly receptive to music that\nwas fetched via the movie and uniformly stated that it im-\nproved the immersiveness of the reading experience, and\neven transported them to the \ufb01ctional world.\nFuture work. While our method has been successful in\ngenerating a soundtrack for a book with a movie adapta-\ntion, it needs to be modi\ufb01ed to make it work on books with-\nout adaptations. Future work can potentially use our ap-\nproach to investigate common trends across books to deter-\nmine new cross-narrative rules. Our approach can also be\nextended for human-in-the-loop collaborative soundtrack\nconstruction applications, though such a use case is beyond\nthe scope of this work.6. ACKNOWLEDGMENTS\nWe thank Siddarth Baasri for his valuable input in\nanalysing motifs present in the Harry Potter soundtrack\nand Saravanan Senthil for illustrating Fig. 1.\n7. REFERENCES\n[1] M. G. Boltz, \u201cMusical soundtracks as a schematic in-\n\ufb02uence on the cognitive processing of \ufb01lmed events,\u201d\nMusic Perception , vol. 18, pp. 427\u2013454, 2001. 1\n[2] M. C. Pennington and R. P. Waxler, Why reading books\nstill matters: The Power of Literature in Digital Times .\nRoutledge, 2017. 1\n[3] M. Che\u0142kowska-Zacharewicz and M. Paliga, \u201cMusic\nemotions and associations in \ufb01lm music listening: The\nexample of leitmotifs from The Lord of the Rings\nmovies,\u201d Annals of Psychology (Polish) , 2019. 1\n[4] J. Salas, \u201cGenerating music from literature using topic\nextraction and sentiment analysis,\u201d IEEE Potentials ,\nvol. 37, pp. 15\u201318, 2018. 2\n[5] H. Davis and S. M. Mohammad, \u201cGenerating mu-\nsic from literature,\u201d in Workshop on Computational\nLinguistics for Literature @ European Association of\nComputational Linguistics (CLfL@EACL) , 2014. 2, 4\n[6] S. Harmon, \u201cNarrative-inspired generation of ambient\nmusic,\u201d in International Conference on Computational\nCreativity (ICCC) , 2017. 2\n[7] M. Thorogood and P. Pasquier, \u201cComputationally\ncreated soundscapes with audio metaphor,\u201d in In-\nternational Conference on Computational Creativity\n(ICCC) , 2013. 2\n[8] A.-M. Oncescu, A. S. Koepke, J. F. Henriques,\nZ. Akata, and S. Albanie, \u201cAudio retrieval with natu-\nral language queries,\u201d in Interspeech , 2021. 2\n[9] M. Won, S. Oramas, O. Nieto, F. Gouyon, and\nX. Serra, \u201cMultimodal Metric Learning for Tag-Based\nMusic Retrieval,\u201d in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2021. 2\n[10] R. Cai, C. Zhang, C. Wang, L. Zhang, and W.-Y . Ma,\n\u201cMusicSense: Contextual Music Recommendation us-\ning Emotional Allocation Modeling,\u201d in ACM Interna-\ntional Conference on Multimedia (ACM MM) , 2007. 2\n[11] M. Won, J. Salamon, N. J. Bryan, G. J. Mysore, and\nX. Serra, \u201cEmotion embedding spaces for matching\nmusic to stories,\u201d in International Society for Music In-\nformation Retrieval (ISMIR) , 2021. 2, 4\n[12] S. Reddy and J. Mascia, \u201cLifetrak: Music in Tune\nwith your Life,\u201d in International Workshop on Human-\nCentered Media (HCM) , 2006. 2[13] L. Baltrunas, M. Kaminskas, B. Ludwig, O. Moling,\nF. Ricci, A. Aydin, K.-H. L\u00fcke, and R. Schwaiger, \u201cIn-\nCarMusic: Context-Aware Music Recommendations\nin a Car,\u201d in International Conference on Electronic\nCommerce and Web Technologies (EC-Web) , 2011. 2\n[14] A. Stupar and S. Michel, \u201cPicasso: Automated sound-\ntrack suggestion for multi-modal data,\u201d in Interna-\ntional Conference on Information and Knowledge\nManagement (CIKM) , 2011. 2\n[15] A. Zehe, L. Konle, L. K. D\u00fcmpelmann, E. Gius,\nA. Hotho, F. Jannidis, L. Kaufmann, M. Krug,\nF. Puppe, N. Reiter, A. Schreiber, and N. Wiedmer,\n\u201cDetecting scenes in \ufb01ction: A new segmentation\ntask,\u201d in European Association of Computational Lin-\nguistics (EACL) , 2021. 2, 9\n[16] S. Sarfraz, N. Murray, V . Sharma, A. Diba,\nL. Van Gool, and R. Stiefelhagen, \u201cTemporally-\nweighted hierarchical clustering for unsupervised ac-\ntion segmentation,\u201d in Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2021. 2, 9\n[17] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y . Liu, \u201cMPNet:\nMasked and Permuted pre-training for Language Un-\nderstanding,\u201d in Advances in Neural Information Pro-\ncessing Systems (NeurIPS) , 2020. 2, 9\n[18] N. Reimers and I. Gurevych, \u201cSentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks,\u201d in\nEmpirical Methods in Natural Language Processing\n(EMNLP) , 2019. 2\n[19] M. A. Hearst, \u201cText tiling: Segmenting text into multi-\nparagraph subtopic passages,\u201d Comput. Linguistics ,\nvol. 23, pp. 33\u201364, 1997. 2, 9\n[20] M. Riedl and C. Biemann, \u201cText Segmentation with\nTopic Models ,\u201d Journal for Language Technology and\nComputational Linguistics (JLCL) , vol. 27, no. 47-69,\npp. 13\u201324, 2012. 2, 9, 10\n[21] E. G\u00f3mez, \u201cTonal description of polyphonic audio\nfor music content processing,\u201d INFORMS J. Comput. ,\nvol. 18, pp. 294\u2013304, 2006. 3, 4\n[22] P. N. Juslin and J. A. Sloboda, Handbook of Music and\nEmotion: Theory, Research, Applications . Oxford\nUniversity Press, 2011. 3, 4\n[23] O. Lartillot, P. Toiviainen, and T. Eerola, \u201cA matlab\ntoolbox for music information retrieval,\u201d in Data Anal-\nysis, Machine Learning and Applications , 2008. 3\n[24] J. Foote and M. L. Cooper, \u201cMedia segmentation us-\ning self-similarity decomposition,\u201d in IS&T/SPIE Elec-\ntronic Imaging , 2003. 3, 4\n[25] M. Tapaswi, M. B\u00e4uml, and R. Stiefelhagen, \u201cStory-\nGraphs: Visualizing Character Interactions as a Time-\nline,\u201d in Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2014. 3, 9[26] M. Tapaswi, M. B\u00e4uml, and R. Stiefelhagen,\n\u201cBook2Movie: Aligning Video Scenes with Book\nChapters,\u201d in Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , 2015. 3, 5, 10\n[27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark\net al. , \u201cLearning transferable visual models from natu-\nral language supervision,\u201d in International Conference\non Machine Learning (ICML) , 2021. 3, 4\n[28] M. Brysbaert, A. B. Warriner, and V . Kuperman, \u201cCon-\ncreteness ratings for 40 thousand generally known\nenglish word lemmas,\u201d Behavior Research Methods ,\nvol. 46, pp. 904\u2013911, 2014. 3\n[29] A. Wang, \u201cAn industrial strength audio search algo-\nrithm,\u201d in International Society for Music Information\nRetrieval (ISMIR) , 2003. 4\n[30] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen,\nG. Nemade, and S. Ravi, \u201cGoEmotions: A Dataset of\nFine-Grained Emotions,\u201d in Association of Computa-\ntional Linguistics (ACL) , 2020. 4\n[31] T. Eerola, O. Lartillot, and P. Toiviainen, \u201cPrediction\nof multidimensional emotional ratings in music from\naudio using multivariate regression models,\u201d in Inter-\nnational Society for Music Information Retrieval (IS-\nMIR) , 2009. 4Supplementary Material\nWe present some additional details on segmentation\n(Sec. A) and alignment (Sec. B). We also provide addi-\ntional details of the reading app (Sec. C) and round up\nthe appendix with the questionnaire used in the perceptual\nstudy (Sec. D).\nA. SEGMENTING THE BOOK, MOVIE, AND\nMUSIC\nA.1 Book Segmentation\nWe segment the book using the hierarchical clustering ap-\nproach described in Sarfraz et al. [16], using the of\ufb01cial\nimplementation2. We extract our text features using the\nsentence-transformers package, which provides\nseveral pretrained models for sentence embeddings. Our\nchosen model, MPNet [17], ranked \ufb01rst on the leader-\nboard as of writing, on sentence embedding and seman-\ntic search tasks. Speci\ufb01cally, we use the model named\nall-mpnet-base-v2 . For our \ufb01nal segments, we use\nthe segments from the third partition, as it avoided the\nover-segmentation present at lower partitions while ensur-\ning that segments were homogeneous largely. A histogram\nof word counts per chapter segment can be seen in Fig. 6.\nAt one point, we also considered annotating the chap-\nters with text segments to evaluate the ef\ufb01cacy of our ap-\nproach but quickly came to realise that it would be mislead-\ning. As described in Zehe et al. [15], segment boundaries\nin \ufb01ction can be attributed to many, often overlapping rea-\nsons. For instance, a shift in time during a \ufb02ashback may\noverlap with a change in location. Further, depending on\nthe differentiating factor used, segments could overlap; a\nsection divided by an emotion may belong to one when\nconsidering time alone. As such, some of our prelimi-\nnary evaluations of this approach, using a few annotated\nchapters, undersold the ef\ufb01cacy of our approach and re-\nported poor F1/accuracy scores. In reality, we \ufb01nd that our\ntext segments are highly plausible and re\ufb02ective of topi-\ncal changes in the text, which was also validated by the\nparticipants of the perceptual study. We leave a thorough\ninvestigation of \ufb01ction text segmentation to future work, as\nit falls outside the scope of this paper.\nWe also tried TextTiling [19], using the NLTK imple-\nmentation3, but found that it uniformly partitioned the text\ntypically, with every chapter containing 3-5 segments re-\ngardless of content (see Fig. 7). In addition, we tried Top-\nicTiling [20] which uses Latent Dirichlet Allocation topic\nmodels to segment text, using its of\ufb01cial implementation4.\nThis produces a series of depth score, indicating the likeli-\nhood of having a segment boundary at that instant. As seen\nin Fig. 8, TopicTiling seemed to over-segment the text, and\nproduced several short segments. Both these methods have\n2https://github.com/ssarfraz/FINCH-Clustering/\ntree/master/TW-FINCH\n3https://www.nltk.org/_modules/nltk/tokenize/\ntexttiling.html\n4https://github.com/riedlma/topictiling\n500 1000 1500 2000\nNumber of words0369121518Number of segmentsFigure 6 . Histogram showing the word count distribution\nfor chapter segments.\n012345678910111213141516\nChapter Number012345Number of Text Segments\nFigure 7 . Number of chunks per chapter of Harry Potter\non using TextTiling [19].\nalso been shown to be perform poorly on a German variant\nof the \ufb01ction segmentation task [15].\nA.2 Movie Segmentation\nWe segment the movie using the shot threading approach\ndescribed in Tapaswi et al. [25], with its of\ufb01cial MATLAB\nimplementation5. As this approach requires pre-computed\nvideo shots, we use an open-source package6to obtain\nshots. For Harry Potter , we obtain 2,525 shots. We also\ntried PySceneDetect for shot/scene detection but found\nthat it resulted in over-segmentation, which is undesirable\nwhen trying to obtain high-con\ufb01dence matches between\nthe text and movie.\nA.3 Music Segmentation\nOur choice of novelty based segmentation is a re\ufb02ection\nof how tracks undergo emotional and tonal shifts. After\nmultiple rounds of testing, we settled on a kernel width of\n5https://github.com/makarandtapaswi/Video_\nShotThread_SceneDetect/\n6https://github.com/makarandtapaswi/\nshotdetection/0 10 20 30 40 50\nParagraph Number0.00.20.40.60.81.0Depth ScoreFigure 8 . Depth score of various segment boundaries,\nas computed by TopicTiling [20], for a single chapter of\nHarry Potter . Higher values suggest the presence of a\nboundary.\n64 for this, as it produced perceptually cohesive segments\nthat were suf\ufb01ciently long.\nB. ALIGNING THE BOOK, MOVIE, AND MUSIC\nB.1 Chapter-Scene Coarse Alignment\nIn order to align book chapters with movie scenes, we\nlargely reproduce the approach described in Tapaswi et\nal. [26]. Their approach parses each chapter and video\nscene to collect a collection of dialogues and characters,\nidenti\ufb01ed by POS tagging on the text/script/subtitles. A\nsimilarity score is then computed between the two on two\ncounts - the similarity of character histograms and length\nof the longest common subsequence (LCS) between movie\nand book dialogues. We refer the reader to the original pa-\nper for more details on its implementation.\nOur implementation did however, differ in a few as-\npects. We align the subtitles with the movie transcript (ob-\ntained online7) using Dynamic Time Warping with LCS\nas the distance function, to obtain speaker identities for all\ndialogues, as the subtitles did not provide this. The original\npaper used facetracks instead.\nTo obtain character names, we process the entire book\nusing BookNLP8, which detects quotes and performs\nspeaker attribution. We only count those characters who\nhave dialogue, unlike in POS Tagging, as they are more\nlikely to be seen in the movie. We use the character names\nobtained here for our histogram. We automatically match\ncharacters names in the movie and book based on LCS, to\nobtain a common space for our character histogram.\nNext, we compute an inverse character frequency to pri-\noritise rare characters who can provide strong alignment\ncues and scale our character histogram accordingly.\nWe set\u000b= 1 (Eq. 6 in Tapaswi et al. [26]) and use\nequal weights for the character and dialogue similarity\nscores. The resulting alignment is visualised in Fig. 9.\n7https://warnerbros.fandom.com/wiki/Harry_\nPotter_and_the_Philosopher%27s_Stone/Transcript\n8https://github.com/booknlp/booknlpB.2 Movie Audio-Soundtrack Alignment\nWe use Shazam API9inPython to identify music from\nthe movie audio. We also tried an open-source imple-\nmentation of audio \ufb01ngerprinting and music identi\ufb01cation,\nDejaVu10, by registering the soundtrack album in a cus-\ntom database. However, this approach failed to effectively\nrecognise music in the presence of noise, though different\nparameters may yield better results.\nC. READING APPLICATION\nIn order to facilitate a smooth reading experience, we built\na custom reading app to ensure that readers with varying\nreading speeds could enjoy our soundtrack. We built our\nreading app using React11and used simple state manipu-\nlation to change the track played based on scroll position.\nWe dynamically cross-fade songs based on scroll position\nusing the Web Audio API.\nD. STUDY QUESTIONNAIRE\nAs part of the perceptual study, we ask our volunteers sev-\neral questions in a semi-structured oral interview:\n1. What is your general feedback with what you\u2019ve\nread/heard?\n2. Compare your reading experience with music to one\nwithout.\n3. How was the music placed with respect to the text /\nHow aligned is the text and music?\n4. Would you say that the the music is repetitive? De-\nscribe your thoughts with respect to the diversity of\nmusic played as well as how often a single song\nplays.\n5. Different music is played at different points in the\nbook, did this have any effect on your reading expe-\nrience?\n6. How did the music transitions affect your reading\nexperience?\n7. How did the music transitions align with changes in\ntext?\n8. Did the soundtrack re\ufb02ect the emotion present in the\ntext? How well did it do?\n9. Were there any instances that you particularly liked?\nWere there any instances that you didn\u2019t like?\n9https://github.com/Numenorean/ShazamAPI\n10https://github.com/worldveil/dejavu/\n11https://reactjs.org/0 20 40 60 80 100\nScene Number0\n5\n10\n15Chapter Number\n0.00.20.40.60.81.01.2Figure 9 . Visualisation of the chapter-scene (17 chapters, 120 scenes) similarity scores computed for Harry Potter . The\nred line on the matrix shows the shortest path on the graph, which aligns the book and movie.",
    "sample": "SONUS TEXERE! AUTOMATED DENSE SOUNDTRACK CONSTRUCTION FOR BOOKS USING MOVIE ADAPTATIONS Jaidev Shrir ...  . The red line on the matrix shows the shortest path on the graph, which aligns the book and movie.",
    "summary": "\n\nHuman:\nCreate a very detailed summary of this paper. Include the main concepts and mathematical equations if any. Additionally, extract the abstract word for word.\n\nAssistant:Here is a detailed summary of the paper:\n\nAbstract (word-for-word):\n\nSONUS TEXERE! AUTOMATED DENSE SOUNDTRACK CONSTRUCTION\nFOR BOOKS USING MOVIE ADAPTATIONS\nJaidev Shriram Makarand Tapaswi Vinoo Alluri\nInternational Institute of Information Technology, Hyderabad\nhttps://auto-book-soundtrack.github.io/\njaidev.shriram@students.iiit.ac.in, makarand.tapaswi@iiit.ac.in, vinoo.alluri@iiit.ac.in\nABSTRACT\nReading, much like music listening, is an immersive ex-\nperience that transports readers while taking them on an\nemotional journey. Listening to complementary music has\nthe potential to amplify the reading experience, especially\nwhen the music is stylistically cohesive and emotionally\nrelevant. In this paper, we propose the first fully automatic\nmethod to build a dense soundtrack for books, which can\nplay high-quality instrumental music for the entirety of the\nreading duration. Our work employs a unique text pro-\ncessing and music weaving pipeline that determines the\ncontext and emotional composition of scenes in a chap-\nter. This allows our method to identify and play relevant\nexcerpts from the soundtrack of the book\u2019s movie adap-\ntation. By relying on the movie composer\u2019s craftsman-\nship, our book soundtracks include expert-made motifs and\nother scene-specific musical characteristics. We validate\nthe design decisions of our approach through a perceptual\nstudy. Our readers note that the book soundtrack greatly\nenhanced their reading experience, due to high immersive-\nness granted via uninterrupted and style-consistent music,\nand a heightened emotional state attained via high preci-\nsion emotion and scene context recognition.\n\nMain Concepts:\n\n- Proposes an automated method to build a dense soundtrack for books using movie adaptations\n- Reading books is an immersive experience similar to listening to music\n- Complementary music can enhance the reading experience if it is stylistically and emotionally relevant\n- The method identifies relevant excerpts from the movie soundtrack to match parts of the book\n- Relies on the craftsmanship of the movie composer for high quality music\n- Validated through a perceptual study showing the soundtrack enhances reading immersion and emotion\n\nKey Methods:\n\n- Segment the book chapters, movie scenes, and soundtrack tracks into smaller coherent chunks\n- Align the book chapters to movie scenes at both coarse and fine levels\n- Extract emotion labels for book segments and music segments\n- Match book segments to movie scenes and extract music playing in those scenes\n- Use emotion matching to fill gaps where book segments don't align to movie scenes\n- Weave together movie-matched and emotion-matched music segments into full soundtrack\n\nOverall, the paper proposes a novel approach to automatically construct an immersive soundtrack for books by leveraging movie adaptations. It aligns the narrative elements across the book and movie in order to lift musically relevant segments from the movie soundtrack. A user study validates that this approach enhances the reading experience.\n\nHuman:Plan a 6 page zine of the paper in a 'idk' style with the aim of creating a zine that offers an overview ( outlines the main objectives and scope). The text and the images of the zine however should be purely academic - covering the main concepts and equations. Do not build a story, analogy or metaphor in the text or image prompts. The image prompts should be very explicit with obviously visualizable wording. The images prompts should all have a consistent color palette.. For each page of the zine, provide thorough text summaries of the concepts discussed (not just description of the text). Additionally, include ideas for images and a description of the style of the images. All images should maintain a consistent style. The zine should begin with a title on the first page and conclude with its abstract on the last page\n\nAssistant:Here is a plan for a 6 page academic zine on the paper:\n\nPage 1 (Title Page):\nSonus Texere! \nTitle in a fun, stylized font. Image could be a stack of books with music notes floating above them. \n\nPage 2 (Introduction):\nText summarizing the introduction and context. Main points:\n- Books are immersive like music\n- Music can enhance reading experience \n- Proposing automated method to build book soundtrack using movie adaptation\n\nImage could be someone reading a book with music notes and movie film floating around them.\n\nPage 3 (Methods - Segmentation):\nText describing the segmentation of book chapters, movie scenes, and soundtrack tracks into smaller chunks. Equations for book and music segmentation.\n\nImage showing book, movie, and music symbols being split into pieces.\n\nPage 4 (Methods - Alignment): \nText on aligning book to movie and movie to soundtrack. Description of coarse chapter-scene and fine paragraph-scene alignment.\n\nImage of book and movie symbols lined up together.\n\nPage 5 (Methods - Soundtrack Assembly): \nText explaining assembling soundtrack using movie and emotion matches.\n\nImage of music notes coming together into a soundtrack.\n\nPage 6 (Conclusion):\nAbstract reprinted verbatim.\n\nImage similar to title page, closing the loop.\n\nAll images use a consistent, academic-looking flat illustration style with solid colors. The text is technical and academic, avoiding storytelling. The zine flows logically from introduction to methods to conclusion.",
    "page_contents": [
        {
            "image_description": [
                "A stack of open books piled on top of each other with musical notes floating up from the pages. The books and notes are illustrated in a flat, solid color art style with bold outlines. The books are muted earth tones of brown, green, and red. The musical notes are black with white accents."
            ],
            "image_phrases": [
                "stack of books, musical notes, flat color illustration"
            ],
            "heading": [],
            "body_texts": [],
            "title": [
                "Sonus Texere! Automated Soundtracks for Books"
            ],
            "abstract": [],
            "catchy_text": []
        },
        {
            "image_description": [],
            "image_phrases": [],
            "heading": [],
            "body_texts": [
                "Reading books is an immersive experience similar to listening to music, as both can transport and emotionally engage readers and listeners. Complementary music has the potential to enhance the reading experience if it is stylistically cohesive and emotionally relevant to the book's narrative. This paper proposes an automated method to build a dense, continuous soundtrack for books using the music from movie adaptations."
            ],
            "title": [],
            "abstract": [],
            "catchy_text": []
        },
        {
            "image_description": [
                "A stack of thick books with chapters visibly divided is shown split into smaller pieces, with each piece having a part label. The art style is flat, academic illustration with solid colors of blue, black and white."
            ],
            "image_phrases": [
                "stack of books, chapters, split into pieces, segmentation"
            ],
            "heading": [],
            "body_texts": [
                "To enable alignment between the book chapters and movie scenes, both modalities are segmented into smaller coherent chunks. The book chapters are split into paragraphs based on punctuation and other delimiters. The movie scenes are partitioned based on shot boundaries and subtitles. This segmentation allows for finer-grained alignment between textual paragraphs and visual scenes."
            ],
            "title": [],
            "abstract": [],
            "catchy_text": []
        },
        {
            "image_description": [],
            "image_phrases": [],
            "heading": [],
            "body_texts": [
                "This method aligns the book chapters to corresponding movie scenes at two levels: coarse and fine. The coarse alignment matches chapters to scenes based on narrative similarity. The fine alignment goes deeper to match paragraphs within chapters to specific scenes and timestamps. By aligning the book to the movie at multiple granularities, the system can identify relevant musical segments from the movie soundtrack to match the current part of the book being read."
            ],
            "title": [],
            "abstract": [],
            "catchy_text": []
        },
        {
            "image_description": [
                "A cartoon illustration of musical notes of different colors floating together to form a continuous musical staff. The notes are brightly colored in shades of blue, pink and yellow. They have smiling faces and little arms and legs as they join together. The style is fun and whimsical."
            ],
            "image_phrases": [
                "smiling musical notes, bright colors, whimsical style, comes together into staff"
            ],
            "heading": [],
            "body_texts": [
                "This page explains how the selected music pieces from the movie soundtrack are woven together into a full immersive soundtrack for the book. The emotion-matching and scene-matching excerpts are seamlessly joined into a continuous musical experience. The aim is to construct an uninterrupted soundtrack that enhances the reading journey."
            ],
            "title": [],
            "abstract": [],
            "catchy_text": []
        },
        {
            "image_description": [
                "A stack of books with musical notes floating above them. The books are illustrated in a flat, solid color academic style, with a palette of blues, greens, and browns. The music notes are flowing up from the books in wavy lines, colored in bright primary shades of red, blue and yellow. There is a warm glow behind the book stack, and the title of the zine \"Sonus Texere!\" is visible on the book spines."
            ],
            "image_phrases": [
                "stack of books, music notes, flowing, wavy lines, bright primary colors, warm glow, zine title visible"
            ],
            "heading": [],
            "body_texts": [],
            "title": [],
            "abstract": [
                "This zine proposes an automated method to build an immersive soundtrack for books using movie adaptations. By aligning narrative elements and lifting music from the movie, the soundtrack enhances the reading experience with continuous, stylistically relevant music that heightens the emotional journey. Reader studies validate the approach's ability to increase immersion and emotion."
            ],
            "catchy_text": []
        }
    ],
    "fonts": "Seagram Tfb",
    "image_styles": {
        "style": "The images use a flat, two-dimensional illustration style with solid colors and clear outlines. The style is clean and minimalistic, with some geometric elements. There is limited perspective or depth. The characters and objects are stylized versions rather than realistic. The color palette is vibrant, using bright primary colors like red, blue, yellow, and green.",
        "style_phrases": "flat two-dimensional solid colors clear outlines clean minimalist geometric stylized characters vibrant primary colors"
    }
}